\head{Сложные линейные алгоритмы}
Сегодня мы продолжим изучение линейных алгоритмов на более сложных примерах. Алгоритмы, которые на предстоит пройти, будут опираться на уже пройденные линейные алгоритмы, поэтому сначала нужно осознать прошлые алгоритмы и только потом переходить к чтению этого раздела.


\subhead{Отрезок с максимальной суммой}
Раньше мы уже рассматривали поиск отрезка с заданной суммой, а теперь мы хотим решить похожую задачу: есть набор $a$ длины $n$ и нам нужно найти такие индексы $1 \leq l \leq r \leq n$, что значение выражения $a_l + a_{l + 1} + \ldots + a_r$ — максимально возможное. При этом элементы могут быть как положительные, так и отрицательные. 

К сожалению, идея двух указателей, которыми мы пользовались при поиске заданной суммы, так просто не сработает, ведь если у нас есть отрицательные элементы, то не известно, как будет меняться сумма при добавлении и удалении элементов: при добавлении она может стать меньше, а при удалении — больше. Поэтому давайте сначала придумаем какое-нибудь решение, а потом попробуем его упростить. Понятно, как решать задачу за \O{n^3} — просто переберём все пары индексов и циклом сложим значения между этими индексами. Но раз мы раньше научились искать суммы на отрезке за \O{1}, то теперь мы можем применить тот алгоритм и в этой задаче, и получить сложность \O{n^2}, потому что перебирать пары индексов всё равно придётся.

Теперь вспомним, что сумма на отрезке $s = a_i + a_{i + 1} + \ldots + a_j$ считается по формуле $s = p_j - p_{i - 1}$, где $p$ — префиксные сумму (кто забыл: $p_i$ — сумма первых $i$ чисел). А теперь заметим, что если индекс $j$ зафиксирован, то поиск оптимального $s$ превращается в поиск наименьшего $p_i$, или же в поиск элементов с наименьшей разностью. А такую задачу мы раньше уже решали!

Поэтому давайте опишем итоговый линейный алгоритм. Во-первых. предподсчитаем префиксные суммы в $p$. А во-вторых, запустим наш алгоритм поиска элементов с наибольшей разностью на этом наборе $p$. Результат выполнения как раз и даст нам оптимальный ответ.

Понятно, что сложность такого алгоритма будет \O{n}, ведь он состоит из двух последовательных частей, каждая из которых выполняется за линейное время.


\subhead{Ближайший меньший слева}
Рассмотрим другую задачу: задан набор $a$ длины $n$, для каждого элемента которого нужно найти ближайший слева меньший элемент, более формально для $a_i$ у нас будет ответ $d_i$, если $a_{d_i} < a_i$ и $a_j \geq a_i$ для $d_i < j < i$. Если же у нас нет такого $d_i$, то будем считать, что ответом должен быть $d_i = 0$.

Мы хотим придумать линейный алгоритм, поэтому, как и в прошлых задачах, на каждом шаге нам будет нужно хранить какую-то полезную информацию, по которой мы сможем находить ответы для следующих шагов. Логично, что хотелось бы хранить такие индексы, которые бы потом смогли стать ответами. Поэтому придумаем, как поддерживать список таких индексов, и что с ним делать.

Пусть на очередном шаге у нас есть список потенциальных ответов $s$ длины $m$: $s_1,\ s_2,\ \ldots,\ s_m$ эти элементы добавлялись последовательно, то есть $s_1 < s_2 < \ldots < s_m$). Но ведь понятно, что если в этом наборе есть какие-то индексы, указывающие на элементы большие или равные очередного $a_i$, то такие элементы точно больше не будут потенциальными ответами, ведь $a_i$ будет меньше их и при этом правее, а значит такие элементы можно удалить. А остальные элементы вполне могут ещё служить ответами, поэтому мы их оставим. Таким образом в наборе $s$ останутся только такие числа, что $a_{s_j} < a_i$, поэтому самый правый элемент и будет нашим $d_i$. А далее нам стоит добавить индекс $i$, ведь $a_i$ может быть ответам для какого-нибудь ещё числа.

Во-первых, заметим, что такой алгоритм будет всё время хранить возрастающую последовательность. А во-вторых, понятно, что если принять $a_0 = -\infty$ и добавить $0$ в $s$, то при работе алгоритма в $s$ всегда будет оставаться $0$, указывающий на $-\infty$, и ответы вида $d_i = 0$ получатся автоматически.

Теперь проверим, что наш алгоритм, хранящий возрастающую последовательность, действительно является линейным. Каждый индекс у нас ровно один раз добавится в $s$ и не более одного раза удалится (может не удалиться). Поэтому на каждый элемент тратится не более 2 операций с $s$, а значит итоговая сложность составит \O{n}. Также стоит отметить, что симметричная задача по поимку ближайшего меньшего справа решается аналогично.


\subhead{Наибольший прямоугольник вписанный в гистограмму}
Рассмотрим ещё одну задачу: есть набор $h$ размера $n$: $h_1,\ h_2,\ \ldots,\ h_n$, при этом $h_i \geq 0$. По ним строится гистограмма (над каждым элементом рисуется столбик соответствующей высоты), и требуется найти наибольший по площади прямоугольник, который можно вписать в эту гистограмму.

Одно решение можно придумать достаточно быстро: переберём все пары индексов $(l, r)$ и будем вписывать наибольший прямоугольник на участке $h_l,\ h_{l + 1},\ \ldots,\ h_r$. При этом если ещё немного подумать, то понятно, что его максимальная высота будет $x = \min\limits_{i = l}^{r} h_i$, ширина $y = r - l + 1$, а площадь $s = x \cdot y$. Если каждый раз минимум искать циклом, то получим сложность \O{n^3}.

Понятно, что такой алгоритм совсем не оптимальный, поэтому заметим в нём одно важное свойство — высота прямоугольника всегда равна одному из $h_i$. а значит прямоугольник можно задавать одним числом $i$, означающим прямоугольник с высотой $h_i$ и наибольшей возможной шириной. Но ведь наибольшая ширина как раз ограничивается двумя элементами, меньшими нашего: $h_l,\ h_r < h_i$ при $l < i < r$. А это значит, что мы можем использовать алгоритм из предыдущей задачи.

И тогда наш итоговый алгоритм будет таким: предподсчитаем $l_i$ — ближайшие меньший слева и $r_i$ — ближайший меньший справа. Тогда для всех индексов $1 \leq i \leq n$ ответом будет $s = h_i \cdot (r_i - l_i - 1)$, и среди этих $s$ нужно выбрать наибольший. При этом нужно как-нибудь аккуратно обработать случаи, когда слева или справа нет меньших элементов, например считать, что в таких случаях $l_i = 0$ и $r_i = n + 1$ соответственно.

Понятно, что такой алгоритм линейный, ведь он состоит из трёх линейных частей (подсчёт $l_i$, подсчёт $r_i$ и поиск ответа), а значит итоговая сложность — \O{n}.


\subhead{Минимум в скользящем окне}
Следующей задачей, которую мы рассмотрим будет минимум в скользящем окне: дан набор $a$ длины $n$ и для каждых $k$ подряд идущих элементов (их называют окном) нужно найти минимум среди них. Конечно, мы могли бы решить эту задачу за \O{k \cdot (n - k)}, перебрав элементы в каждом окне, но такой метод не оптимальный и мы хотим придумать линейный алгоритм. Поскольку мы уже изучили много линейных алгоритмов, то придумаем целых 3 решения этой задачи.

\textbf{Способ №1.} Вспомним, что раньше мы уже реализовали очередь с поддержкой минимума. Поэтому мы можем сначала добавить $k$ первых элемента и узнать минимум среди них. А после этого мы будем добавлять очередной элемент в конец, удалять один элемент из начала и получать ответ для нового окна.

Понятно, что сложность такого алгоритма \O{n}, ведь все операции с очередью мы реализовали за \O{1}.

\textbf{Способ №2.} Воспользуемся идеей, которая у нас была раньше: будем хранить индексы текущих перспективных элементов в наборе $d$. Понятно, что последователь значений $a_{d_i}$ должна строго возрастать, ведь если найдётся убывающий фрагмент, то $i < j$, $d_i < d_j$ и $a_{d_i} > a_{d_j}$, а значит когда мы теперь будем сдвигать наше окно вправо, то оба элемента будут присутствовать в окне до тех пор, пока $d_i$ не выйдет за границу окна, а следовательно $a_{d_i}$ не перспективный, так как всегда будет $a_{d_j}$, который меньше.

Таким образом у нас получится следующий алгоритм: сохраним в $d$ возрастающую последовательность для первых $k$ элементов (такое мы раньше уже делали). Дальше на каждом шаге будем удалять первый элемент, если его индекс вышел за пределы окна. После этого обработаем добавление нового элемента $x$ справа: пока последний элемент $d$ больше или равен, чем $x$, то он не перспективный и его можно удалить. В конце добавим $x$ в конец $d$ и возьмём ответ для текущего окна из начала $d$ (у нас же там хранится возрастающая последовательность, а значит минимум в её начале).

Стоит отметить, что если использовать структуру данных \lcpp{deque}, то она сможет обработать все операции с $d$ за \O{1} и итоговая сложность алгоритма составит \O{n}.

\textbf{Способ №3.} Будем использовать задачу, где мы искали ближайший справа элемент, меньший данного. Найдём для нашего набора $a$ ответ на ту задачу и сохраним его в набор $r$.

Теперь заведём один указатель $i$, который будет указывать на текущий минимум в окне, изначально присвоим $i = 1$. А теперь, пока $p_i$ будет указывать на элемент окна, будем перемещать наш указатель: $i = p_i$, и понятно, что если мы больше не можем переместить указатель, то уже нашли минимум в окне. А теперь остаётся сдвинуть окно: если минимум старого окна находится в новом, то мы можем делать те же операции, что и в начале, и так найдётся минимум для нового окна; если же старый минимум вылез за границы окна, то поступим как в начале: присвоим $i$ первый индекс окна и будем делать $i = p_i$, пока не вылезем из окна.

Может показаться, что такой алгоритм не является линейным, но это не так, ведь наше $i$ каждый раз перемещается только вправо, а значит, таких перемещений будет не больше, чем $n$, а, следовательно, сложность такого алгоритма будем \O{n}.
