\head{Структуры данных, часть 1}
Мы уже прошли достаточно много разных алгоритмов, и поэтому сделаем небольшой перерыв в их изучении. Сегодня мы будем рассматривать \term{структуры данных}, позволяющие решать некоторые задачи асимптотически быстрее.


\subhead{DSU}
Первая структура данных, которая мы пройдём это \term{disjoint-set-union (DSU)}, также известная, как \term{union–find data structure}, а в русскоязычном варианте — \term{Система непересекающихся множеств}. Структура нужна, чтобы оптимально обрабатывать три операции со множествами: создание множества из одного элемента ($make(x)$), получение множества для элемента ($find(x)$) и объединение двух множеств ($merge(a, b)$). При этом, изначально в структуре в каждом множестве по одному элементу. На практике мы часто будем знать количество элементов $n$ в DSU до всех операций объединения, поэтому вместо (помимо) $make(x)$ нам понадобится $build(n)$, которая по своей сути будет вызывать $make(i)$ для всех $i = 1,\ 2,\ 3,\ \ldots,\ n$, где $n$ — как раз количество элементов в структуре.

Конечно, хотелось бы использовать встроенные структуры данных. Но оказывается, что это не эффективно, ведь если хранить каждое множество как \lcpp{set} (или отсортированный \lcpp{vector}, что для наших целей тоже подходит), то на операцию $find$ нам в худшем случае придётся посмотреть на все текущие множества, что займёт \O{n}, в худшем случае, когда каждый элемент в отдельном множестве. Также медленно будет работать и операция $merge$, ведь когда объединяемые множества будут иметь суммарный размер порядка \O{n}, то потребуется столько же операций. Поэтому использовать встроенные структуры достаточно неэффективно.

Основная идея, которой мы будем пользоваться, это представление данных в виде дереве. Хоть графы мы ещё и не изучали (а дерево — это как раз термин из теории графов), но сама суть дерева довольно понятно: у нас есть корень, который никуда не ведёт и все остальные вершины ведут в одну какую-то вершину так, что в итоге все эти пути приходят в корень. Нагляднее будет представлять дерево, как одну вершину сверху (корень) и от корня ведут рёбра в низ, к другим вершинам, от них ещё в низ и так далее, причём в каждую вершину ведёт только одно ребро сверху, и идёт сколько-то рёбер вниз, а вбок рёбра не идут.

В компьютере же мы будем представлять как список $p$, в котором $p_i$ соответствует вершине, в которую мы попадём, если пойдём от $i$ вверх. При этом для корня будем хранить какое-то специальное значение, самое удобное это $p_r = r$, где $r$ как раз и есть корень.

Всё это конечно хорошо, но пора бы и переходить к самой структуре данных. Будем представлять каждое множество в DSU как дерево. При этом каждое дерево мы будем характеризовать его корневой вершиной (также называется представителем или лидером). Тогда для ответа на запрос $make(x)$ мы будем делать отдельное дерево для $x$: $p_x = x$. Для запроса $find$ будем подниматься по дереву до его корня $r$ и возвращать $r$. А если же нам требуется объединить два дерева, заданных какими-то своими вершинами, то мы корень одного дерева подвесим к корню другого, сделав $p_a = b$ (можно было бы подвешивать и не к корню, но это нам понадобится чуть позже). И в целом, можно уже написать реализацию, которая бы работала:

\cpp{ds-1}{22}

Вот только есть одна проблема, она всё ещё не эффективна, ведь дерево может случайно выродиться в длинную цепочку и тогда все операции у нас будут за \O{n}, что медленно. Поэтому нужно прибегнуть к двум \term{эвристикам}: одна будет ускорять операцию $find$, а другая ускорит $merge$ и в итоге после этих двух оптимизаций получится что-то хорошее. Далее мы рассмотрим эвристики по отдельности и в конце узнаем, какую итоговую сложность мы всё же получим, если будем использовать несколько эвристик вместе.

\textbf{Эвристика \term{сжатие путей}.} Раз у нас дерево может вырождаться в длинную цепь, то от этого стоит избавиться. И это действительно можно сделать, ведь нам же не важно, как выглядит дерево, главное, чтобы все его вершины оставались в дереве. Поэтому давайте те вершины, которые расположены далеко от корня, будем поднимать вверх, делая их непосредственным родителем сам корень.

Но делать это постоянно не хочется, ведь тогда всё время придётся проверять все вершины. Поэтому давайте будем переподвешивать вершины только когда это ничему не мешает, то есть во время операции $find$ будем поднимать все посещённые вершины наверх:

\cpp{ds-1}{4}

А с тернарным оператором тело функции можно сократить до одной строки:

\cpp{ds-1}{1}

И такая эвристика уже даёт нам сложность \O{\log n} в среднем на один запрос

\textbf{Эвристика \term{Random-Union}.} В общем-то теперь хочется как-то оптимизировать операцию $merge$, потому что вдруг у нас одно из деревьев маленькое, а второе большое. Тогда если мы подвесим большое дерево к маленькому, то у нас много путей до корня станет на $1$ больше, а вот если мы к большому дереву подвесим маленькое, то только небольшое количество путей увеличится на 1. Но, чтобы не заморачиваться с размерами деревьев, то будем просто подвешивать деревья в случайном порядке :)

\cpp{ds-1}{5}

Такой подход действительно эффективно работает на случайных данных, давая сложность \O{\log n}, но вот если запросов на объединение большого дерева с маленьким много, то такая эвристика даёт ускорение всего в два раза\footnote{Точнее в два раза уменьшается математическое ожидание времени работы, ведь для алгоритмов со случайными числами скорость работы зависит от самих этих чисел.}.

\textbf{Эвристика объединения по рангу.} Но всё же можно придумать честные способы, как подвешивать именно маленькое дерево к большому. Для этого будем дополнительно хранить для каждой вершины её ранг $rank_i$. Понятно, что в начале у нас все вершины одинаковые, поэтому будем считать, что $rank_i = 1$.

Вариант первый, ранговая эвристика на основе размера дерева, она же Union-By-Size. Будем поддерживать $rank_i$ как количество вершин, находящихся в дереве с корнем в $i$:

\cpp{ds-1}{8}

Вариант второй, ранговая эвристика на основе глубины дерева, или же Union-By-Height. Будем поддерживать $rank_i$, как максимальную длину пути из $i$ вниз, в какую-то вершину:

\cpp{ds-1}{8}

Оказывается, что оба варианта ранговой эвристики тоже дают нам сложность \O{\log n} в среднем (что для эвристики по высоте, ведь высота увеличивается на 1 при увеличении дерева в два раза).

\textbf{Объединение эвристик.} Понятно, что если использовать несколько эвристик вместе, то итоговая сложность операций с DSU уменьшится. Также понятно, что нужно выбрать по одной эвристики для $find$ и $merge$ (ведь не понятно, как объединить несколько вариантов для $merge$).

Также можно заметить, что Random-Union эвристика продолжает работать, если ей использовать одновременно с сжатием путей. Аналогично, эвристика по размеру дерева тоже работает, ведь для корней деревьев $rank_i$ будут подсчитываться правильно, а для других вершин $find$ может их сломать переподвешиванием, но это и не важно, ведь их ранг нигде в коде не используется. С эвристикой по высоте не всё так очевидно, ведь $find$ меняет высоту деревьев, но это не страшно, так как теперь $rank_i$ превращается в верхнюю оценку для глубины, которая при этом была когда-то достижима, а значит выбор дерева в $merge$ останется правильным.

Поэтому давайте ещё раз представим DSU с эвристиками:

\cpp{ds-1}{25}

И в такой реализации все операции в среднем занимают \O{\ac{n}}, где $\ac{n}$ — обратная функция Аккермана. Но важным свойством $\ac{n}$ является то, что она растёт ооочень медленно и для всех разумных чисел (меньших $10^{10^{10^{19500}}}$ :)) она даёт значения меньше пяти. Поэтому фактически можно считать, что операции с DSU выполняются за константное \O{1} время.


\subhead{Sqrt-декомпозиция}
Теперь изучим корневую декомпозицию, основанную на идеи \term{разделяй и властвуй} и на формуле $\frac{n}{\sqrt{n}} = \sqrt{n}$. Что же это может значит?

А это означает, что $n$ входных элементов мы можем разбить на $\sqrt{n}$ блоков по $\sqrt{n}$ элементов в каждом (только эти корни нужно округлить до целых чисел). И теперь мы сможем обрабатывать запросы на диапазонах элементов. Для этого каждый запрос обработает \O{\sqrt{n}} блоков, полностью вошедших в диапазон, и \O{\sqrt{n}} элементов, которые попали в боковые блоки запроса. И таким образом мы сможем обработать запрос за \O{\sqrt{n}}. При этом на хранение элементов и блоков нам понадобится всего \O{n} памяти (\O{n} на элементы и \O{\sqrt{n}} на блоки).

Давайте, чтобы было понятнее, рассмотрим задачу, которую у нас раньше не получалось решить. Пусть дано $n$ элементов $a_1,\ a_2,\ \ldots,\ a_n$ и требуется ответить на $q$ запросов поиска минимума на отрезке $[l; r]$: $x = \min(a_l,\ a_{l + 1},\ \ldots,\ a_r)$.

Понятно, что мы будем делать в такой задачи. Разобьём этот набор на блоки и для каждого блока вычислим минимум. А после этого для всех поступающих запросов будем вычислять минимум на основе предподсчитанных ответов для блоков. Посмотрим, как это будет выглядеть в коде:

\cpp{ds-1}{28}

Вот так относительно просто реализуется корневая декомпозиция. Единственное, что может вызывать затруднения, это вычисление индексов при ответе на запросы, но это можно легко сделать, если нарисовать нашу sqrt-декомпозиция на листочке. Кроме того, можно заметить, что мы нигде не пользуемся специальными свойствами операции минимума. Поэтому с помощью этого же кода можно решить задачу, в которой требуется найти сумму или НОД на отрезке, лишь поменяв в нужных местах значения $INF$ и функцию по пересчёту ответа (поменять с минимума на сумму).

Но кроме универсальности к операции sqrt-декомпозиция обладает возможностью быстрого обновления элементов! Для этого нам достаточно поменять значение самого элемента и пересчитать ответ для блока, в котором располагается элемент, следовательно обновление делается за \O{\sqrt{n}}. В коде же это тоже выглядит достаточно просто:

\cpp{ds-1}{7}

Но, на самом деле, и это ещё не всё, что умеет sqrt-декомпозиция. Потому что на самом деле корневая декомпозиция — это не структура данных, а образ мысли!

Если подходить более формально, то кроме разбиения элементов массива на блоки, также может быть полезно и разделение запросов на блоки. Рассмотрим же мы это на примере усложнения DSU, в котором также поддерживается и удаление рёбер. А именно: в начале имеется пустой граф из $n$ вершин, и поступает $m$ запросов трёх типов: провести ребро $(a, b)$, удалить ребро $(a, b)$ и проверить, есть ли путь между $a$ и $b$.

Следует заметить, что если бы операций удаления рёбер не было, то задача бы решалась с помощью обычного DSU. Ведь проведение ребра это тоже самое, что и операция $merge$ в DSU, а проверку на наличие пути между вершинами можно сделать через сравнение представителей для вершин (которые возвращаются из $find$). Но вот удаление рёбер DSU никак не может поддерживать, ведь DSU думает, что работает с деревом, а на самом деле граф в этой задаче может иметь циклы и деревом не являться.

Поэтому давайте будем действовать так: все операции, кроме удаления рёбер обработаем с помощью DSU, а удаление обработаем отдельно. Для этого разобьём все $m$ запросов на $\sqrt{m}$ блоков по $\sqrt{m}$ элементов и при этом будем поддерживать список рёбер, проведённых на данный момент (идеи, как именно это сделать, можно почерпнуть из разделов про графы).

В начале каждого блока мы построим DSU по тем рёбрам, которые уже есть и при этом не удалятся в текущем блоке. Далее каждый запрос добавления ребра мы сделаем с помощью DSU, а на каждое удаления ребра, будем просто удалять его из списка рёбер. Когда же нам потребуется проверить, связаны ли две вершины, мы сначала узнаем множества, в которых находятся наши вершины, а после этого с помощью алгоритма обхода графа (о них будет позже) проверим, есть ли связь между вершинами по рёбрам. которые удалены из DSU но фактически ещё не удалены.

Можем попытаться оценить сложность представленного выше алгоритма (список рёбер будем хранить в \lcpp{unordered_set}, в котором все операции за \O{1} в среднем). На построение DSU понадобится \O{\sqrt{m} \cdot m \ac{m}}, на добавление рёбер \O{m \ac{m}}, на удаление рёбер \O{m}. Проверки же на связность займут \O{m \sqrt{m} \ac{m}}, потому что обход графа займёт \O{\sqrt{m}}, а на каждом шаге будет нужно делает $find$ для концов ребра. И итоговая сложность алгоритма будет \O{m \sqrt{m} \ac{m}}, что очень даже хорошо.
