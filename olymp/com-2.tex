\head{Сортировки}
Теперь перейдём к изучению сортировок. Задача сортировки заключается в том, что нам дан набор чисел, и его нужно упорядочить по какому-то признаку, мы будем сортировать в порядке не возрастания. При этом, разрешается использовать только операцию $<$. Для таких алгоритмов есть одна важное ограничение — они не могут работать быстрее, чем за \O{n \log n}. Чтобы сортировать объекты быстрее, придётся использовать дополнительную информацию о них, и такую сортировку мы тоже рассмотрим. Конечно, все алгоритмы сортировки описать очень сложно, поэтому ниже будет представлена только их часть.

\subhead{Bogosort}
Первая из сортировок работает дольше всего, аж за \O{n \cdot n!} (хотя это и ожидается из названия \term{глупая сортировка}).

Но при этом у ней очень простой алгоритм: переберём все возможные перестановки массива и проверим, является ли текущая перестановка отсортированной. 

\subhead{Selection sort}
Название переводится, как \term{сортировка выбором}.

Суть сортировки такова: на первом шаге выберем наименьший элемент массива и поставим его на первое место. На втором шаге — выберем наименьший элемент из всех, кроме первого и обменяем его со вторым элементом. И так далее будем выбирать элемент на очередную позицию, а если минимальных элементов будет больше одного, то выберем из них самый первый.

Сложность такого алгоритма составит \O{n^2}.

\subhead{Insertion sort}
Следующая сортировка — \term{сортировка вставками}.

Алгоритм её таков: будем накапливать отсортированный массив в начале данного. Первый элемент можно пропустить — он точно образует отсортированный массив. Для второго мы посмотрим, где он должен стоять: если после первого, то ничего менять не будем; если перед первым, то первый элемент сдвинем на второе место, а этот поставим на первой, то есть как бы вставим его на первую позицию. И дальше будем действовать так для всех элементов — те числа из отсортированной части, которые больше, чем новое значение — сдвинем вправо на одну позицию и поставим это новое значение перед ними.

Сложность такого алгоритма будет \O{n^2}.

\subhead{Bubble sort}
Это \term{сортировка пузырьком}. Она, также как и предыдущие, имеет сложность \O{n^2}.

Но при этом, её алгоритм чуть более прост: будем по очереди смотреть на пары соседних элементов: сначала на первый и второй, потом на второй и третий, ..., в конце на предпоследний и последний. Если на очередном просмотре окажется, что пара упорядочена неправильно, то элементы в ней нужно поменять местами. Таким образом за один проход по массиву самый большой элемент точно окажется на самой правой позиции (всплывёт, как пузырёк :)). После этого будем совершать ещё итерации, тем самым накапливая в конце массива его упорядоченную часть. При этом понятно, что уже упорядоченные элементы можно повторно не просматривать, также понятно, что если за одну итерацию не произошло никаких изменений, то алгоритм можно завершать.

\subhead{Tree sort}
Теперь начнём изучать чуть более оптимальные сортировки и начнём с \term{сортировки деревом}.

Концепция у этой сортировки очень простая: вспомним, что есть встроенные структуры, который умеют оптимально хранить отсортированные данные с помощью дерева (в C++ такие структуры — \lcpp{set} и \lcpp{map}). Просто сохраним в них все элементы, они сами их отсортируют, а мы после этого запишем данные в отсортированном виде.

Но простота этого алгоритма таит в себе дополнительные расходы (времени и памяти), так как работать с деревом не так легко, как с обычными массивами. Но всё же сложность у такой сортировки будет \O{n \log n}, потому что все операции с деревом выполняются за \O{\log n}, а у нас таких операций \O{n}.

\subhead{Merge sort}
Она же \term{сортировка слиянием}.

Её алгоритм таков: массив из одного элемента точно отсортирован, поэтому ничего делать не нужно. Если элементов больше, то разобьём их на две примерно равных части (если длина чётная, то поровну; если не чётная, то в одной части будет на 1 элемент больше). После этого отдельно отсортируем каждую часть этим же алгоритмом, а далее научимся сливать два отсортированных массива в один. Для этого, будем запоминать два индекса — позиции в первой и второй частях, начиная с которых элементы ещё не сливались. Далее на каждом шаге выбираем наименьший из элементов, на которые указывают индексы, сохраняем его в общий отсортированный массив и сдвигаем этот указатель. Когда один массив целиком обработан (и его указатель стал указывать на конец), то оставшийся можно скопировать в общий отсортированный массив.

Сложность такого алгоритма составляет \O{n \log n}, потому что с каждым из \O{n} элементов совершается \O{\log n} операций по его копированию во время слияния массивов.

\subhead{Quick sort}
С английского — \term{быстрая сортировка}.

Действует эта сортировка так: на каждом шаге будем выбирать элемент массива. Дальше нужно получить массив, в котором в начале бы шли элементы не больше выбранного, потом выбранный и в конце — элементы не меньше выбранного. После этого для обеих частей нужно применить эту же сортировку. Остаётся понять, как упорядочить массив нужным образом. Для этого заведём два указателя — один на первый элемент, второй — на последний. Если так получилось, что они указывают на инверсию (левый — на элемент, больший выбранного, правый — на элемент, меньший выбранного), то нужно поменять местами эти два элемента и сдвинуть указатели внутрь массива. Если оба элемента стоят правильно, то также можно сдвинуть оба указателя внутрь. Если оба элемента меньше, чем выбранный, то мы двигаем левый указатель, так как правый элемент точно нужно переставить. Если оба больше — то двигаем правый. Остановиться нужно, когда указатели станут указывать на один элемент или же перескочат друг через друга.

При этом случай, когда элемент равен выбранному можно рассматривать как угодно — хоть действовать с ним так, как будто он больше, хоть так, как будто он меньше, хоть чередовать эти варианты. Выбирать элемент тоже можно произвольно — хоть первый, хоть последний, но считается, что лучше выбирать случайный элемент.

Видно, что у этого алгоритма сложность не так явно оценивается, но утверждается, что она \O{n \log n} в среднем случаем, \O{n} — в лучшем и \O{n^2} — в худшем.

\subhead{Counting sort}
Та самая сортировка, которая использует дополнительную информация и не основывается на операции $<$; переводится с английского как \term{сортировка подсчётом}. Дополнительное ограничение, которое накладывается — сортировать мы можем только числа, при этом из ограниченного диапазона длины $k$, и сложность будет \O{n + k}.

Суть очень простая: заведём ещё один массив длина $k$ и заполним его нулями. После этого будем поочерёдно смотреть на элементы (за \O{n}), которые нужно отсортировать, и прибавлять 1 к соответствующему элементу массива. И после этого мы посмотрим на все элементы нашего массива подсчёта и будем знать, сколько каких чисел встречалось. А значит мы сможем восстановить исходный массив за \O{k}.

При этом на $k$ накладываются ограничения. Во-первых, память должна позволить разместить $k$ элементов; во-вторых, если $k$ очень большое, то такой алгоритм может не уложиться в отведённое время (но, вероятно, ограничение по памяти существеннее, чем по времени).

\subhead{Оптимальная сортировка}
Теперь покажем, почему нельзя сортировать быстрее, чем за \O{n \log n}.

\begin{box-proof}
    Пусть алгоритм совершил $k$ операций сравнения. Поскольку у нас операция сравнения это $<$, а даёт она лишь два варианта ответа, то всего возможных разных вариантов сравнений могло получиться $2^k$. При этом, нужно отличать каждый вариант перестановки массива (их всего $n!$), поэтому должно выполняться неравенство $2^k \geq n!$, или же $k \geq \log_2 n!$.

    Теперь осталось это каким-то образом оценить. Для этого будем использовать \term{Формулу Стирлинга}:
    $$\lim\limits_{n\to \infty} \frac{n!}{\sqrt{2\pi n} \left({ \frac{n}{e} }\right)^{n} }=1 \text{, или } n! \sim {\sqrt{2\pi n}} \left({ \frac{n}{e} }\right) ^{n}$$

    Теперь прологарифмируем по основанию 2:
    $$k \geq \log_2 n! \sim \frac{2n\log(n) - 2n + \log(2 \pi n)}{\log(4)} = O(n \log n) + O(n) + O(\log n) = O(n \log n)$$

    Собственно, мы и получили нужное равенство $k \geq O(n \log n)$, а значит, сложность \O{n \log n} действительно оптимальна для алгоритмов сортировки.

    Также можно сделать эту оценку чуть проще:
    $$k \geq \log_2 n! = \log_2(1) + \log_2(2) + \ldots + \log_2(n) \geq $$
    $$ \geq \log_2\left(\frac{n}{2}\right) + \log_2\left(\frac{n}{2} + 1\right) + \ldots + \log_2(n) \geq \frac{n}{2} \cdot \log_2 \left( \frac{n}{2} \right) = O(n \log n)$$
\end{box-proof}
